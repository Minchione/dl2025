
\documentclass{article}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage[margin=1in]{geometry}

\title{Lab 1 Report: Gradient Descent}
\author{Nguyen Hoang Minh}
\date{\today}

\begin{document}

\maketitle

\section*{1. Introduction}

Gradient Descent is an iterative optimization algorithm used to find the local minimum of a function.
In this lab, we minimize the quadratic function:

\[
f(x) = x^2
\]

Its derivative is:

\[
f'(x) = 2x
\]

We begin at a starting point and update the value of \( x \) iteratively using the gradient descent rule.

\section*{2. Algorithm}

The general formula for updating \( x \) is:

\[
x_{\text{new}} = x_{\text{current}} - \text{step\_size} \times f'(x_{\text{current}})
\]

Where:
\begin{itemize}
    \item \( x_{\text{current}} \) is the current value of \( x \)
    \item \texttt{step\_size} is the learning rate
    \item \( f'(x_{\text{current}}) \) is the derivative at that point
\end{itemize}

\section*{3. Methodology}

In order to demonstrate the gradient descent process:
\begin{itemize}
    \item Define the function \( f(x) = x^2 \) and its derivative \( f'(x) = 2x \)
    \item Start from an initial point \( x = 99 \)
    \item A step size (learning rate) of 0.2 (used)
    \item The algorithm iterates 10 times to approach the minimum
\end{itemize}

\section*{4. Results}

With the condition:
\begin{itemize}
    \item Starting point: \( x = 99 \)
    \item Step size: 0.2
    \item Number of iterations: 10
\end{itemize}

The output values per iteration are:

\begin{center}
\begin{tabular}{ccc}
\toprule
\textbf{Step} & \textbf{x} & \textbf{f(x)} \\
\midrule
1 & 59.40 & 3528.36 \\
2 & 35.64 & 1270.21 \\
3 & 21.38 & 457.28 \\
4 & 12.83 & 164.62 \\
5 & 7.70 & 59.26 \\
6 & 4.62 & 21.33 \\
7 & 2.77 & 7.68 \\
8 & 1.66 & 2.76 \\
9 & 1.00 & 1.00 \\
10 & 0.60 & 0.36 \\
\bottomrule
\end{tabular}
\end{center}
\end{document}
